{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20220321 Day6-2 colab03c 打造自己的 tokenizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sglhsr/Machine_Learing/blob/main/20220321_Day6_2_colab03c_%E6%89%93%E9%80%A0%E8%87%AA%E5%B7%B1%E7%9A%84_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpRN4o4n4EMf"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxc6kSls4X4t"
      },
      "source": [
        "1. 讀入相關套件\n",
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "Tokenizer:把文字訓練成數字\n",
        "我們最主要是需要 `tf.keras` 中的 `Tokenizer`。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyV0Igeq5-OE"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8-74Y0n6TBH"
      },
      "source": [
        "我們使用 `urllib.request` 的 `urlretrieve` 把在 GitHub 上的一個檔案當成在自己硬碟上的檔案使用, 讀入《紅樓夢》。另外, 做好的 Tokenizer 要存起來 (很重要! 很重要! 很重要!) 這裡用 Python 標準的 `pickle` 儲存。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bf7U1Dow7ghZ"
      },
      "source": [
        "from urllib.request import urlretrieve\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CZhSDOu8I95"
      },
      "source": [
        "### 2. 讀入我們收集到所有的文本\n",
        "\n",
        "我們收集許多資料, 比如說粉專上的留言討論。這裡要做的就是把所有文字合併成一個檔案。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZZ0ixCs8lVH",
        "outputId": "e9640c74-f06a-4ccc-ca05-798764946665"
      },
      "source": [
        "urlretrieve(\"https://github.com/yenlung/Deep-Learning-Basics/raw/master/data/dream.txt\", \n",
        "            \"dream.txt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('dream.txt', <http.client.HTTPMessage at 0x7f3137deb550>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcjfRG0B9LTJ"
      },
      "source": [
        "然後就可以像開正常檔案般使用。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m10Zc_a79UK2"
      },
      "source": [
        "f = open('dream.txt', 'r')\n",
        "lines = f.readlines()\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ-Djlsq95O0"
      },
      "source": [
        "這裡面就會是一段段《紅樓夢》。察看後發現, 每段開頭會有用於縮排的 \"\\u3000\\u3000\" 我們想要去掉。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ASCHMao-qDU"
      },
      "source": [
        "text_lines = [x.lstrip('\\u3000\\u3000') for x in lines]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vVeqItjAmtu"
      },
      "source": [
        "再把每一段整合起來。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p10WD5xAAxNk"
      },
      "source": [
        "text = ''.join(text_lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgZeYu40AyM2"
      },
      "source": [
        "### 3. 打造我們的 tokenizer\n",
        "\n",
        "Tokenizer 本身也是個函數學習機, 因此就標準的建構、訓練、預測 \b(使用) 三部曲。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-izv6EnqBw3U"
      },
      "source": [
        "#### 第一部曲: 打造 tokenizer 函數學習機\n",
        "\n",
        "設 `char_level=True` 意思是每一個字 (包括標點), 都有一個代號 (token)。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qn7q-AftCbQp"
      },
      "source": [
        "tokenizer = Tokenizer(char_level=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae4pM-BbClOz"
      },
      "source": [
        "#### 第二部曲: 訓練\n",
        "\n",
        "就把所有文字丟進去就好。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0n7guH2Cuw7"
      },
      "source": [
        "tokenizer.fit_on_texts([text])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb99fQXJCyAF"
      },
      "source": [
        "#### 第三部曲: 預測 (使用)\n",
        "\n",
        "我們可丟一句話 (texts) 進去, 就可回傳對應的一串代碼 (sequence) 回來。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "em9joFNaDuc2",
        "outputId": "0fe947a3-75f0-4d00-fc93-2727845da3db"
      },
      "source": [
        "tokenizer.texts_to_sequences([\"我打造了一個函數學習機。\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[15, 99, 721, 3, 6, 26, 597, 362, 1061, 912, 2]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxP3nnqED_Uu"
      },
      "source": [
        "反過來也可以 sequences to texts。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWmMH8v9Evzr",
        "outputId": "03af5a31-bfa3-45dc-8673-165da7bccd46"
      },
      "source": [
        "tokenizer.sequences_to_texts([[15, 99, 721, 3, 6, 26, 597, 362, 1061, 912, 2]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['我 打 造 了 一 個 數 學 習 機 。']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIc7RHeuFSmT"
      },
      "source": [
        "我們也發現，沒有訓練過的字出現就忽略不計。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sq8IQ-nLE12a"
      },
      "source": [
        "### 4. 把我們的 tokenizer 存起來"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWUh9zFEFi2F",
        "outputId": "a6ed39e0-e02d-4037-cb11-e7d8606479cc"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_4lNrxlF_M_",
        "outputId": "85174214-73b4-48f9-940b-ce975a675aef"
      },
      "source": [
        "%cd \"/content/drive/MyDrive/Colab Notebooks/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igw9qiUSGM77"
      },
      "source": [
        "f = open('MyTokenizer.pkl', 'wb')\n",
        "pickle.dump(tokenizer, f)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om-FG_cmGfxf"
      },
      "source": [
        "之後要用 `pickle` 讀回我們訓練好的 tokenizer 是這樣:\n",
        "\n",
        "```python\n",
        "f = open('tokenizer.pkl', 'rb')\n",
        "tokenizer = pickle.load(f)\n",
        "f.close()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.sequences_to_texts([[15], [99], [721], [3], [6], [26], [], [597], [362], [1061], [912], [2]])"
      ],
      "metadata": {
        "id": "aMYteRbuJPFp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}